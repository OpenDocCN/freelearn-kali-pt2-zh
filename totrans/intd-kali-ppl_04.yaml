- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kali Linux and the ELK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve gained a basic understanding of the evolution of cybersecurity
    as a professional field of study and practice, let’s begin to unpack the Kali
    Purple toolset. You’ll recall our explanation of red and blue colors creating
    purple on the color wheel. That’s because Kali Purple’s genealogy is a double-pronged
    utility coming from two suites of technical tools, one associated with the **red
    team** and the other with the **blue team**. We provided an overview of each grouping
    in the previous chapter. Those lists of tools were not nearly an exhaustive, or
    complete, list of tools – just the highlights.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to briefly explain Kali Linux for those who might
    be delving into Linux for the first time. A popular phenomenon has been developed
    with Kali Purple in that its defensive security offerings are causing some people
    to pursue experience with the Linux **operating system** (**OS**) for the first
    time in their careers. However, most folks who’ve picked up this book likely already
    have Linux experience, and some may even have Kali experience. So, while briefly
    covering Kali Linux, we will focus mostly on unpacking the initial elements of
    the **security information and event management** (**SIEM**) system, the **ELK
    stack**.
  prefs: []
  type: TYPE_NORMAL
- en: '**ELK** stands for **Elasticsearch, Logstash, and Kibana**. They are three
    unique open source software offerings that are usually pasted together for technology
    solutions related to log management, data storage, search/query, data analysis,
    and data visualization. They alone do not define a SIEM. However, when combined
    with certain other core components, such as **Beats** and **X-Pack**, the ELK
    stack will deliver unto us the information and event management system we are
    questing for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple tools available in Kali Purple, but we will be focusing
    on the main powerhouse of the suite, the distributed analytics engine known as
    Elasticsearch. For us to understand the tools we will be working with, we will
    need to focus on the following Elastic elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X-Pack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the SIEM is the core solution you will find within a SOC, it is critical
    that we fully explore and understand this tool, how it works, how and where it
    gets its data, and the different ways we can work with that data to accomplish
    our security objectives. The typical SIEM takes data from multiple other sources,
    normalizes it, enriches it, organizes it, stores it, and then presents it to cybersecurity
    analysts in a unified format for human evaluation and action. Organizations rely
    on it to centralize their defensive security operations. By the end of this chapter,
    you will have a solid understanding of those concepts.
  prefs: []
  type: TYPE_NORMAL
- en: This understanding will come in handy throughout the next three chapters as
    we negotiate the process of preparing our technology for Kali Purple and then
    acquiring, installing, and configuring our technology along with Kali Purple itself.
    Knowing the data flow as it relates to the ELK stack and Kali Linux environment
    will provide the foundation you need to confront any anomalies that might arise
    when we unpack our *SOC-in-a-Box* solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of Kali Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch, Logstash, and Kibana (ELK stack)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of Kali Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though a groundbreaking technology, Kali Purple has its modern roots back in
    the year 1969\. It was then in AT&T’s Bell Laboratories that Ken Thompson and
    Dennis Ritchie co-created the UNIX OS. Ritchie is also famously known for creating
    the C programming language. Nearly all modern computing exists because of Ritchie’s
    two biggest contributions. The original Windows was programmed in C. UNIX eventually
    became available in both open source and commercial varieties. The two biggest
    customers of commercial UNIX were the United States **Department of Defense**
    (**DOD**) and – drumroll please – Apple Computer. That’s right. The Mac OS X is
    built from UNIX. That leaves only one major player to account for… Linux.
  prefs: []
  type: TYPE_NORMAL
- en: A computer scientist at the University of Helsinki in Finland, Linus Torvalds
    grabbed one of the open source versions of UNIX and began to add his style to
    create a brand-new OS based on UNIX. He then publicly released the OS for free.
    *Linus + UNIX = Linux.* Because Mr. Torvalds released the code and a license allowing
    anybody to take it, modify it, and redistribute their own versions of it, Linux
    rapidly grew in popularity, with countless varieties of the OS available in the
    world today.
  prefs: []
  type: TYPE_NORMAL
- en: The Kali Linux genealogy was reinforced by Dennis Ritchie a second time when
    he left Bell Labs to go work for the University of California at Berkley. It was
    there that he eventually created a newer version of UNIX that he dubbed **Berkeley
    Software Distribution** (**BSD**). It was this version of UNIX that Debian Linux
    – the OS based directly on Torvald’s initial contributions – utilized to beef
    up its own powerful OS.
  prefs: []
  type: TYPE_NORMAL
- en: Highly reputable American information security company Offensive Security funded
    the development of Kali Linux to support ethical cybersecurity needs through digital
    forensics and penetration testing. Offensive Security employees Mati Aharoni and
    Devon Kearns took one of the company’s previous Linux distributions and revamped
    it to give the world the Debian Linux derivative now known as Kali Linux.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, the flagship of the SOC is the SIEM, and Kali Linux
    gave us the tools to put that together with the release of Kali Purple in December
    2022\. Let’s start examining the most potent of these tools, the ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch, Logstash, and Kibana (ELK stack)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ELK stack refers to the three software components that collaboratively enrich,
    index, and visualize data for analysis. Some folks will include the Beats family
    of data collection agents as part of the ELK stack. We will cover Beats in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elasticsearch is the central engine of the ELK stack. As we’ll discuss in a
    moment, Elasticsearch is technically a type of non-relational, NoSQL database.
    Without the ability to effectively search through the data we have collected,
    how on Earth would we ever be able to decide if there is something that should
    be alerted to? Kibana would have no path forward to properly display the visualization
    threats it provides to us. It is a very powerful component of the ELK stack in
    that it is designed to handle very large volumes of data in scenarios where a
    lot of querying frequently occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch accomplishes this by using a technique known as **sharding**.
    This is a database terminology that, in simple terms, allows data to be structured
    and organized by breaking it into pieces. This allows the technology to spread
    pieces of data horizontally across multiple partitions/nodes and servers while
    maintaining its relationship with similar data through indexing. These smaller
    chunks of data are called shards. The process of placing the data into shards
    occurs during the indexing process.
  prefs: []
  type: TYPE_NORMAL
- en: The value of creating these shards is that Elasticsearch can then use a technique
    called **parallel processing** to access multiple shards simultaneously. Parallel
    processing is when more than one processing unit – a CPU, for example – works
    concurrently across multiple nodes. It provides the effect of having more than
    one computer working on the same thing at the same time to get the job done quicker.
    This is typically employed in database technology.
  prefs: []
  type: TYPE_NORMAL
- en: Something beneficial Elasticsearch offers is the support for the creation of
    duplicates – **replicas** – of the shards. As implied, these are copies of the
    original shards. This is done to support the *availability* component of the cybersecurity
    framework by creating redundancy, which, in turn, serves the purpose of fault
    tolerance. It creates this safety net by placing the replica shards in a different
    node within the data cluster. Just as the original shards are created during the
    indexing process, so too are the creation of the duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may not sound brag-worthy to someone unfamiliar with searching technology,
    Elasticsearch offers **full-text searching**. That’s a big deal! Many searching/querying
    technologies only perform searches based on matching extracted keywords or embedded
    metadata. Full-text searching means that Elasticsearch will search for and retrieve
    results based on the entire contents of a document and text. That means considering
    the full contents of the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are six key features Elasticsearch utilizes to support this style of
    searching:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization**: Tokenization, when applied to full-text searching, might
    also be considered a form of sanitizing the text. What this means is that Elasticsearch
    breaks the text up into individual units. It then stores the small units, or tokens,
    in a reverse index – sometimes called an **inverted index**. That is done to also
    allow searching based on keywords or simple terms. Part of the tokenization process
    involves steps such as converting all uppercase characters into lowercase characters,
    removing punctuation, and dividing the input into smaller units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Term-based queries**: Term-based queries are just as they sound – a search
    that allows the users to state keywords or very specific terms they want Elasticsearch
    to seek out within a document. It accomplishes this by utilizing the tokenization
    method described previously to help narrow down the results, making sure they
    are directly relevant to the search terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full-text matching**: Elasticsearch utilizes mathematical algorithms to assist
    in result accuracy by employing a technique known as full-text matching. This
    is not to be confused with full-text searching, which we will talk about in a
    moment. Elasticsearch will attempt to determine the relevance of a document as
    it relates to the search query by counting the number of occurrences of terms
    in a document that match one of the search terms. It will also use algorithms
    such as **Term Frequency-Inverse Documents Frequency** (**TF-IDF**) and **Best
    Match 25** (**BM25**) to assess the importance of these search terms within the
    entire collection of documents searched because of measuring those occurrences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full-text search options** (**FTSO**): Another style called FTSO is a related
    concept that differs in its focus and overall functionality. While the matching
    style focuses on exact matching, FTSO takes this a step further by also considering
    word proximity, term frequency, language analysis, relevance scoring as well as
    partial matches, synonyms, and fuzzy matches. It allows for Boolean operators
    commonly found in database languages (**AND**, **OR**, and **NOT**). In simple
    terms, the matching style tends to focus only on providing results where the words
    are an exact match with less of a consideration as to whether those results are
    relevant. FTSO places more effort into making sure the results aren’t simply word
    matches but relevant content matches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language analyzers**: Have you ever wondered how technologies such as Elasticsearch
    deal with information that it finds in a language other than its native English?
    Believe it or not, it can accomplish that feat because Elasticsearch has integrated
    a variety of language analyzers. That includes analyzers that emphasize features
    other than spoken languages. Here are a few examples of some of the more popular
    language analyzers Elasticsearch uses:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard analyzer**: Grammar-based tokenization for most languages'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple analyzer**: Focuses on non-letter characters and lowercase normalization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whitespace analyzer**: Tokenizes each word that is separated by whitespace'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keyword analyzer**: Tokenizes the entire input string'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spoken languages**: English, French, German, Spanish, and many others'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faceted searching and aggregations**: Finally, Elasticsearch offers an advanced
    searching style known as faceted searching and aggregations, which involves complex
    data analysis. Faceted searching is sometimes called **faceted navigation**. This
    is a method that filters data based on different attributes of the data. These
    different filtering dimensions are sometimes called facets, hence the name. Some
    examples of these facets could be things such as color, price range, and shopping
    category. This style provides an avenue for the user to determine how they might
    like to refine their search. It allows the user to drill down to very precise
    levels of searching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Working together with faceted searching is aggregated searching. This is done
    at the beginning of the search to build the most robust supply of potentially
    relevant information for the faceted search to surgically discover the information
    most needed by the user. Three primary types of aggregations are utilized by Elasticsearch:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Bucket aggregations**: Using a logical partition to group documents with
    related attributes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric aggregations**: Performs calculations on the data within each bucket'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline aggregations**: A secondary aggregation that operates based on output
    from other aggregations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to having a larger pool of considerations for the search, Elasticsearch
    also offers data indexing in near real time. One of the methods it uses to deliver
    that feature is something called **inverted indexing**. Whereas traditional indexing
    is when data is stored in a database and is organized based on the total document,
    inverted indexing organizes the data based on the words and terms within the documents.
    The index is built while they are initially parsed and analyzed before being placed
    into an optimized data structure for storage. That structure is the index itself.
  prefs: []
  type: TYPE_NORMAL
- en: To further support near-real-time indexing, Elasticsearch first writes a document
    to a transaction log called a **Write Ahead Log** (**WAL**) immediately before
    the indexing process begins. This is done to protect the data in the case of a
    system failure. Even a brief hiccup in machine function could corrupt the indexing
    process. So, by using a WAL, Elasticsearch can immediately recover the data and
    begin the indexing process again.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of data accuracy, Elasticsearch refreshes the indexes every second.
    When this occurs, it reinforces the availability of the information in the index
    for search operations. That process then gives way to near-real-time searching,
    making the overall process of retrieving information by Elasticsearch incredibly
    efficient in nature. This is a necessary component for the success of any application
    that depends on quick access to information for analytics, such as an SIEM.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logstash is part of the ELK stack with capabilities for ingesting data, transforming/enriching
    it if needed, as well as transporting it from one place to another. While Elasticsearch
    is the core of the ELK stack, Logstash offers some parallel abilities and serves
    as the intermediary that supplies Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logstash can ingest data from a vast array of data sources. Some of these sources
    might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Log files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application programming** **interfaces** (**APIs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet of Things** (**IoT**) devices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After ingesting the data, Logstash offers something called **data transformation**
    and **data enrichment**. This is a critical feature for any analytic application
    that alerts on anomalies. It performs these actions through the use of filters.
    These filters allow users to manipulate incoming data and then send it to the
    destination, where it will become accessible to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: There are many filters, around 50, but we are going to focus on some of the
    simpler and more popular filters here. Feel free to dig in and research new filters
    as you develop and hone your ELK stack skillset. In the meantime, let’s cover
    a few that folks will start learning and training with.
  prefs: []
  type: TYPE_NORMAL
- en: One such filter is the **Grok Filter**. Logstash uses this to parse any type
    of structured log data – which Grok derives from unstructured data – that seems
    to follow a pattern. This action allows Logstash to also extract fields it deems
    meaningful from any unstructured or semi-structured log messages. It uses regular
    expressions to try and recognize and define patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The **Date Filter** is another transformation tool. As you might expect, this
    tool is used to parse and standardize incoming log data with date and time. You
    can customize the date format patterns you want it to parse. This filter is quite
    useful for any log files that contain any sort of date or timestamp. Any properly
    generated log file should have such stamps on them.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a **Translate Filter**, which performs data lookups against external
    mapping files. The filter uses this information to enrich the data coming in through
    the logs. This filter allows you to define key-value pairs within a dictionary
    file. The filter can then enrich the data by matching and replacing specific values
    with corresponding dictionary values.
  prefs: []
  type: TYPE_NORMAL
- en: The aptly named **Mutate Filter** provides Logstash with a variety of operations
    that it can use to manipulate and modify data. With this filter, you can rename
    fields and add or remove them. You can also convert their types and modify the
    values contained within the fields. You can even do things such as split and trim
    strings or concatenate one string with another.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to add geographical information to the data, you will use the **GeoIP
    Filter**. It accomplishes this by performing IP address lookups. By mapping IP
    addresses to corresponding geographic locations, you can enrich your data with
    helpful information such as city, state, and country, as well as latitude and
    longitude.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash uses a **User-Agent Filter** to parse strings typically coming from
    weblogs, which enables it to extract important details about the tools used on
    the endpoint. This could include device type, OS, web browser brand, and version.
    Having this information can allow an analyst to gain a powerful insight into the
    devices and browsers used on either end of the data flow.
  prefs: []
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation** (**JSON**) is one of the most commonplace code-related
    language formats found in defensive security tools. You are not likely to ever
    find a mainstream SIEM or other cybersecurity product that doesn’t have compatibility
    with JSON and for that matter, at least an option or two to present some of the
    information that’s utilized by the application in JSON format. Logstash is no
    exception. It can easily handle JSON formatted data and is even capable of handling
    nested JSON structures. A nested feature in any programming or coding environment
    is when such a feature is found included within itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Spreadsheet fans will be excited to learn that Logstash offers a **CSV Filter**.
    This filter allows Logstash to parse and manipulate comma-separated values. That
    is the common format that’s used when a spreadsheet user extracts and/or stores
    data that can be presented in a spreadsheet manner. This filter handles a variety
    of configurations, such as delimiter, header, row, and column mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Another format that is both commonly found and compatible across different systems
    is **eXtensible Markup Language** (**XML**). Logstash has an XML filter to allow
    it to parse data that comes in XML formats. This filter can extract specific elements
    and/or attributes out of XML documents and then convert them into structured fields
    that can be used later for processing.
  prefs: []
  type: TYPE_NORMAL
- en: The full collection of filters exceeds this list. These are just the most common
    and most likely to be used by Logstash. The complete filter collection makes Logstash
    a very robust data, normalization, and enrichment utility. As you might have picked
    up by now, Logstash is designed in a manner that allows it to deal with both structured
    and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: In dealing with all this data and all the different ways we’ve discussed how
    Logstash manipulates and enriches it, there is one critical coding style of a
    feature that it uses to control the data flow. Those who’ve worked with coding
    and/or software development before know of this type of data control flow as **conditionals**.
    Conditionals allow the user to control the flow based on certain specific conditions
    and criteria. Some might recognize these as an **if-else** syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a fun example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve already touched on, Logstash is a bit of a liaison – a middleman or
    intermediary. That means it isn’t only relied upon for ingesting log information,
    it’s also heavily relied upon to enrich and forward that data with the primary
    output highway being Elasticsearch. It can output data in various formats but
    the most common are CSV and JSON, which we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: There is a plugin for Logstash that C programmers will immediately recognize
    called **stdout** which stands for **standard output**. This plugin allows the
    data to be sent directly to the command line in console applications. The main
    reason for outputting data in that manner is to allow engineers working the Logstash
    instance to have the ability to immediately recognize certain data elements for
    debugging and application testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular output destination is **Apache Kafka**, which is a distributed
    event streaming platform application that was originally developed by the popular
    social media platform LinkedIn. It was released as open source software under
    the Apache Software Foundation. It has earned a reputation for being a credible
    fault-tolerant and efficient utility capable of handling large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we talked about a means of data organization within Elasticsearch
    called the **bucket**. Logstash can also output to the organization that took
    this data organization principle and popularized it on a grand scale: Amazon.
    They did this via their **Simple Storage Service** (**S3**) buckets. It is extremely
    popular in Amazon’s cloud services and is fully integrated with **Amazon Web**
    **Services** (**AWS**).'
  prefs: []
  type: TYPE_NORMAL
- en: Logz.io is a popular cloud-based log management platform that Logstash is compatible
    with outputting to. While it is a log management and analytics platform, it does
    not focus on real-time monitoring, correlation between raw and enriched data sources,
    and security event analysis. Therefore, it falls just a neuron short of being
    considered a full-fledged SIEM.
  prefs: []
  type: TYPE_NORMAL
- en: Another Logstash output integration is **Java Database Connectivity** (**JDBC**).
    JDBC is a Java API that allows developers to interact with relational databases
    using the SQL DB language. It provides interfaces and classes that are standard
    for performing database interactions such as connecting, executing SQL queries,
    updating, or processing results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Redis** is a similar storage option in that it can be used as a database,
    cache, or message broker. Logstash outputs this in-memory data structure storage
    as well. Something unique about Redis is that it stores its primary data in RAM
    to allow for lightning-fast reading and writing operations. It supports a very
    wide range of data types. It utilizes common database methods such as a key-value
    pair model. This is where the data is stored as a pair with a unique key to identify
    them. It focuses heavily on the use of keys and data structures such as strings,
    lists, sets, hashes, and sorted sets. The appeal of Redis is the incredibly fast
    and efficient way it manipulates, stores, and recalls data.'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kibana is the ELK stack utility that helps users explore, visualize, and analyze
    the data that’s been collected and aggregated by the other components. It is designed
    to specifically work with Elasticsearch to provide a user-friendly interface for
    the end user. The most direct term to describe Kibana is simply **data visualization**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data visualization in Kibana occurs from many methods and styles but there
    are seven primary methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Charts**: Charts are probably the most logical expectation here. They include
    line charts, bar charts, area charts, pie charts, scatter plots, and others. This
    gives the users who are more visual learners an opportunity to see potential anomalies
    from a different perspective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maps**: While many software applications will make use of maps, Kibana takes
    this to a higher level. Users can create choropleth maps, symbol maps, and maps
    based on grid coordinates. A **choropleth map** is a type of thematic map that
    focuses on a specific theme or topic. The choropleth style is to use different
    colors or patterns to represent geographical areas/regions. Usually, that would
    be countries, states/provinces, or counties/shires. You have probably seen many
    of these types of maps in your childhood classrooms. Symbol maps are similar but
    instead focus on using symbols to identify and represent specific geographical
    features, while grid coordinate maps are popular when aligned with latitude and
    longitude.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timelion**: Timelion, as the name suggests, is a data visualization plugin
    for Kibana that allows end users to manipulate and analyze time-based data. Like
    most utilities associated with the ELK stack, Timelion can be used for data transformation,
    aggregations, and mathematical calculations. You can perform averages, percentiles,
    and sums, among other functions. It allows the use of variables, which is handy
    in dealing with irregular time series. The typical visualization that’s created
    by Timelion comes in the form of a line chart. When applied to technology, line
    charts are nearly always present in the form of an *X* and *Y*-axis, where the
    *X*-axis is representative of time and the *Y*-axis represents the data values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time Series Visual Builder** (**TSVB**): The TSVB is a visualization option
    in Kibana that focuses on the end user’s comfort. It allows them to build complicated
    time series visualizations with a simple drag-and-drop interface. It gives access
    to a variety of chart types, mathematical aggregations, and custom styling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gauge and metric**: Kibana’s version of gauge and metric visualizations is
    directed at providing visuals that represent a single value within a specific
    range. This is to allow users to present such simple values in a variety of ways,
    including sparklines. **Sparklines** are small and condensed line bar charts or
    sometimes simple line charts that are meant to display variations in data or trends
    within a condensed space. They’re meant to give a quick visual *shape* or look
    at the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heat maps**: One of the more popular data visualizations in the business
    world today is heat maps. Heat maps are two-dimensional and often present in the
    form of a global or other large geographical map. They are popularly used to display
    trends, correlations, and patterns to help analysts solve complicated and/or wide-ranging
    problems. One of the more popular heat map usages in the world of cybersecurity
    is to show the origins of cyberattacks or source IP locations. Typically, heat
    maps have carefully integrated color schemes so that when multiple entities cross
    paths, it increases the intensity of the color, such as when some mobile phone
    GPS maps darken the route color from blue to yellow and eventually to red, depending
    on how congested the traffic is in a particular location. Heat maps use labels
    and annotations, but the best ones will keep this concise and limited to keep
    the visualization appearing clean to the observer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tag clouds**: Tag clouds, also known as **word clouds**, are visualizations
    of text data that is usually shown with words in different sizes and colors based
    on category and prominence. They tend to measure word prominence based on frequency.
    The algorithms that generate these sorts of maps usually perform a bit of pre-processing
    before deciding on the final output. They do this to remove common words that
    are not likely related to data, such as prepositions and pronouns. This is a form
    of text sanitization, similar to tokenization, which is seen in Elasticsearch
    text-matching methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another type of visualization offered by Kibana is **dashboard creation**. This
    is the overarching element that is most associated with Kibana and the one that
    makes use of the different charting and other visualizations we’ve just explored.
    A Kibana dashboard provides a centralized view where multiple visualizations can
    be combined, and queries and filters can be applied to create a fully customized
    layout. They help track what is known in the business world as **key performance
    metrics** (**KPIs**) in near real time.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana provides support for users to perform ad hoc searches. This is done across
    indexed data in Elasticsearch but with the benefit of a visual search interface.
    You can construct queries using simple search syntax if you like. The more advanced
    users have the option of using the Lucene query syntax, which is helpful for precise
    searches. The search results are displayed instantly and then you can further
    refine, filter, and sort the data if you wish to continue drilling down into specific
    subsets of information.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of queries and filters, Kibana provides a way to visually build queries
    and filters so that you can deeply explore data. Kibana’s **domain-specific language**
    (**DSL**) allows users to construct complex queries and aggregations to extract
    very deep and meaningful insights from the data. You can use filters to narrow
    down data based on specific criteria, such as time range, attributes, or custom-defined
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the greater strengths Kibana has is its ability to excel at time-based
    analysis. This allows you to explore data trends, patterns, and anomalies more
    thoroughly over a certain period. You can adjust the time ranges, set custom time
    intervals, and even apply histograms with dates so that you can analyze data at
    different levels of granularity. The time picker enables you to specify the time
    range you wish to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: '**Geospatial analysis** is another feature that’s offered that allows you to
    visualize and analyze location-based data. You can use this feature to plot data
    on interactive maps, geocode IP addresses, apply geospatial aggregations, and
    visualize density using heatmaps. A nice bonus from Kibana is the support for
    multiple map layers, custom base maps, and geographic shape files. It also offers
    geospatial extensions, the most common being the Elastic Maps Service.'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana allows users to set up alerts and monitors to track specific events or
    conditions in real-time data. You can define alert rules based on a variety of
    factors, including data thresholds, patterns, or other conditions. It can also
    work with external notifications such as email and the popular webhook, among
    others, as the alerts are triggered. It is this set of features that takes the
    ELK stack one step closer to an authentic SIEM environment than most log ingestion
    and manipulation suites.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana isn’t only a data visualization tool – it also offers an element of security.
    It provides a very robust set of security features to control access to data.
    It does this by supporting authentication and authorization mechanisms, **role-based
    access control** (**RBAC**), and integration with external authentication providers
    such as **Lightweight Directory Access Protocol** (**LDAP**) and/or the **Security
    Assertion Markup** **Language** (**SAML**).
  prefs: []
  type: TYPE_NORMAL
- en: The ELK stack, as presented thus far, works with data manipulation/enrichment,
    visualization, and overall processing. However, we need to have a way of gathering
    this information and sending it to the ELK stack in the first place. Enter Beats.
  prefs: []
  type: TYPE_NORMAL
- en: Agents and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a couple of prominent additional applications that are associated
    with the ELK stack. They are the open source Beats and the commercially available
    X-Pack. It is the addition of these two components that transitions the ELK stack
    into a fully functioning SIEM. Together, they provide data collection and shipping,
    alerting and notification, machine learning for detecting hidden anomalous behavior,
    and automated reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Beats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beats is a group of data collection and transportation agents. These are sometimes
    referred to as **data shippers**. They are lightweight – miniature – applications
    that are installed on endpoints so that they include personal computers, servers,
    and other network devices for the sole purpose of collecting data to ship off
    to Elasticsearch and Logstash for further processing in real time. Beats collect
    operational data from the devices they are installed on. It gets it from different
    sources, including logs and network traffic, along with other relevant information.
    The Beats family includes various specialized data shippers, such as **Filebeat**
    for log files, **Metricbeat** for machine metrics, **Packetbeat** for network
    data, and **Auditbeat** for audit events, among others. Beats uses lightweight
    agents installed on servers or systems to efficiently collect data with minimal
    resource usage. Let’s look at these in some detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filebeat**: Filebeat is designed for collecting and shipping log files. It
    can monitor log directories and files. It takes the data that it collects and
    ships it off to Elasticsearch or Logstash, where it will be enriched and/or further
    processed and analyzed. Filebeat supports different log file formats, including
    plain text logs, JSON logs, syslog, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metricbeat**: This beat collects system-level metrics and statistics, such
    as CPU usage, memory utilization, disk I/O, and network metrics among others.
    It can gather metrics from various sources, such as OSs, services, containers,
    databases, and cloud platforms. Just like Filebeat operates, Metricbeat ships
    the collected metrics to Elasticsearch or Logstash for enrichment and further
    processing, storage, analysis, and visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Packetbeat**: Wouldn’t it be great to capture these types of metrics from
    network data? That’s where Packetbeat comes into play. It captures network data
    and analyzes various protocols, including HTTP, DNS, MySQL, and Redis, along with
    many other protocols so that it can extract metadata and behavioral insights.
    Packetbeat’s data collection targets mean that it can provide near-real-time visibility
    into network traffic. This might include transaction details, request and response
    data, or network errors. Packetbeat can also be used for troubleshooting performance
    issues, identifying security threats, and monitoring network activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auditbeat**: This one is designed to collect and ship audit events from the
    Linux Audit Framework and Windows Event Logs. Auditbeat monitors system logs for
    audit events so that it can provide insight into user activities and look for
    unauthorized escalation of privileges by monitoring privilege changes, filesystem
    modifications, and others. Auditbeat has the potential to be a key player in identifying
    a compromised network and/or device. It is an invaluable tool that helps with
    compliance monitoring, security analysis, and detecting suspicious activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a package, Beats allows for some data transformation capabilities so that
    it can prepare the collected data for analysis. Beats provides lightweight functions
    that are applied to events before they’re sent on to Elasticsearch or Logstash.
    These functions are called Beats processors. They can enrich, filter, and modify
    the collected data. These processors allow operations such as filtering fields,
    enriching data with metadata, renaming fields, and much, much more.
  prefs: []
  type: TYPE_NORMAL
- en: One critical consideration for business technologies is the ability to adapt
    to sudden and/or unexpected rapid business growth. Thankfully, Beats is built
    to handle scalability. Not only that, but it is also designed to operate in high-volume
    and distributed environments. Beats can be easily scaled horizontally through
    the deployment of multiple Beats agents across different servers or systems. These
    agents provide configurable options to deal with load balancing, fault tolerance,
    and failover. This ensures data collection with high availability and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Being an integral contribution to the Elastic stack, Beats’ design allows it
    to seamlessly integrate with the other components of the ELK stack – Elasticsearch,
    Logstash, and Kibana. They enable the efficient collection, transport, and processing
    of data before it is stored in Elasticsearch for further analysis and visualization.
    Beats can be configured to send data directly to Elasticsearch or Logstash for
    additional data transformation and enrichment.
  prefs: []
  type: TYPE_NORMAL
- en: That last part brings up a topic we should address. We’ve discussed a few times
    now that data can be sent directly to either Elasticsearch or Logstash and that
    Beats is also capable of sending it to either one. It’s important to examine these
    scenarios so that as we advance throughout our careers, we can select the appropriate
    configurations when setting up the data shippers.
  prefs: []
  type: TYPE_NORMAL
- en: That said, let’s consider that sending data directly to Elasticsearch or instead
    to Logstash as an intermediary both has its advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of sending data directly to Elasticsearch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: Sending data directly to Elasticsearch means there’s one less
    stop the data needs to make before reaching its destination. By eliminating Logstash
    as an intermediary, we are keeping the data pipeline simplified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: As we talked about just a short while ago, modern technology
    in business needs to be able to adjust to sudden and/or unexpected business growth.
    By directly sending data to Elasticsearch, we are creating an allowance for horizontal
    scaling as well. That will make it possible to distribute the workload across
    multiple Elasticsearch nodes. This significantly improves performance and gives
    us the ability to handle larger volumes of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Any time a stop is removed from a data flow, less processing
    overhead will be required. There is also going to be less lag/latency due to the
    ability of the data to keep moving past the point where a stop would otherwise
    be located. This gives us the near-real-time data ingestion and indexing of data
    that Elasticsearch is known for. That, in turn, leads to faster availability for
    search and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlined architecture**: Supporting the greater efficiency argument, having
    Beats, or any data shipper for that matter, make a direct integration with Elasticsearch
    means we have a tighter and more straightforward architecture. That keeps the
    entire data flow simpler, which, in addition to improved performance, also means
    less surface for breaks and flaws.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of sending data directly to Elasticsearch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited data transformation**: We may be more efficient but we are also missing
    out on the benefits of Logstash. This means that by sending data directly to Elasticsearch,
    we no longer have the data enrichment and transformation capabilities that Logstash
    has to offer. Elasticsearch still has these values but not at the same level as
    Logstash. Logstash provides much more powerful filtering, enrichment, and data
    parsing capabilities so that it can preprocess and transform the data before indexing
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity in certain scenarios**: There are going to be scenarios where
    the additional preprocessing of data would simplify the work Elasticsearch needs
    to do in the end. So, there are times when there will be enrichment that is better
    suited for Logstash. Ultra-complex data pipelines, especially those involving
    many data sources or more advanced transformations, would benefit better from
    the flexibility and capabilities of Logstash.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility with non-Elasticsearch systems**: There could be times when
    systems other than Logstash or possibly even other than Beats will ship data to
    Elasticsearch. If any of these systems do not integrate fully with Elasticsearch,
    there could be difficulties with normalizing the data being transferred. Any data
    coming from Logstash is certain to be compatible with Elasticsearch. Logstash,
    on the other hand, is set up with a multitude of plugins to assist with transforming
    data into a format that is compatible with Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The advantages of using Logstash as an intermediary are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation and enrichment**: Logstash possesses very robust data
    processing capabilities that are simply greater in size and scope than Elasticsearch
    has to offer. Logstash is capable of advanced filtering, enriching, and parsing
    of data. It also has some alerting capabilities. It can support the more advanced
    styles due to its vast array of plugins and filters. Collectively, these abilities
    allow Logstash to transform data into a compatible format before passing it onto
    Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility and integration**: The vast array of plugins and filters that
    exist in Logstash allows for integration with many data sources and destinations.
    Logstash can support many different formats, protocols, and systems because of
    this. That also makes it compatible with many non-Elasticsearch systems, which
    it can grab data from and translate for Elasticsearch. This gives us what some
    would call data pipeline versatility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility in complex scenarios**: Whereas Elasticsearch is not the best
    suited for extremely complex scenarios, Logstash excels in this area. These areas
    include complex data pipelines, and situations where data needs to be collected
    from multiple sources, transformed, combined, or split before being passed on
    to Elasticsearch. This increases overall granularity and flexibility when dealing
    with diverse requirements and data manipulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of using Logstash as an intermediary are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased complexity**: While Logstash does expand the ability to normalize
    many different types of data for Elasticsearch, it also introduces an additional
    stop in the data flow by adding a new component to the pipeline. That’s one more
    thing that must be configured, managed, and refined. This can increase the complexity
    and overhead of the system, which is especially concerning for smaller deployments
    that likely have simpler requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Potential performance impact**: Anytime an additional component is added
    to any technological pipeline, this means there will be an additional need for
    processing and overhead. This will reduce efficiency and increase the latency/lag
    of the data flow. In cases where there is a large volume of data flow and/or a
    large number of transformations and enrichments needed, there is likely to be
    a performance hit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overhead and resource utilization**: Because there is extra technology, there
    is also going to be extra demand for the resources. That means more stress on
    the CPU, memory, and disk space compared to sending the data directly to the Elasticsearch
    integration. This might be more valuable while considering scenarios involving
    very high volumes of data or if the previously mentioned resources are limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The general gist of Elasticsearch versus Logstash data ingestion is that sending
    data directly to Elasticsearch improves efficiency while reducing capabilities
    and sending it to Logstash first has the exact opposite effect of increasing capabilities
    while reducing efficiency. Elasticsearch greatly improves the overall efficiency
    and reduces preprocessing overhead. Logstash offers a plethora of data transformations
    where it can take data not compatible with Elasticsearch in its native format.
    Then, when it’s done processing it, it delivers it to Elasticsearch as a compatible
    ingestion platform. In the end, the decision about where to send the data is going
    to depend on how you feel about the expected complexity of the data pipeline,
    the need for data transformation, and compatibility with other systems in your
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: X-Pack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whereas the four components of the ELK stack we’ve discussed thus far are all
    free, open source applications, X-Pack is a partially free extension that offers
    additional paid services. It offers enhanced capabilities over the completely
    free applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has five distinct offerings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: X-Pack delivers security through several actions. One offering
    is RBAC, which allows you to define and manage roles and privileges for users.
    This provides a strict level of control over documents, indexes, and other resources.
    It provides **Transport Layer Security** (**TLS**) encryption for secure communication
    between clients and Elasticsearch. X-Pack also offers secure user authentication
    by utilizing methods such as LDAP, Active Directory, and native authentication.
    This helps keep access to the Elasticsearch cluster safe and secure. It also has
    an audit logging feature so that it can log security-related events to help you
    monitor and track user and system actions for anomalous behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: X-Pack provides detailed monitoring of Elasticsearch clusters.
    Some of the issues it monitors include node health, resource usage, and indexing
    rates. This is so that you can identify and respond to performance issues and
    bottlenecks. It will also look at query performance, identifying slow-responding
    queries and inefficient search patterns. You can set up monitoring alerts based
    on predefined conditions and thresholds. These alerts will provide notifications
    to you if a predefined condition or metric is met.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting**: One of the most critical components of X-Pack is its alerting
    capabilities. It uses an application called **Watcher** to define and schedule
    alert rules based on query results and aggregations. Like all the big players
    in SIEM technology, it can perform actions such as providing automatic email notifications
    or Slack messages. It can also execute custom scripts, giving analysts the power
    to proactively respond to critical events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning (ML)**: X-Pack offers anomaly detection through its ML capability.
    This enables the automatic detection of anomalies in time series data, which assists
    analysts in identifying unusual patterns, deviations, or other anomalous behavior.
    It also uses ML to forecast future trends based on historical patterns that would
    otherwise be hard to detect or are hidden altogether. Something interesting about
    X-Pack’s ML is that it helps to identify what it believes to be the most influential
    factors that contribute to anomalous behavior. This helps investigators and incident
    responders with their examination of root cause analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reporting**: Finally, X-Pack allows you to create and schedule reports, which
    is a well-rounded feature that, like alerting, will help transition your ELK stack
    into a full-fledged SIEM. It offers these reports in a variety of formats, such
    as **.pdf**, **.csv**, and **.xls**, which works wonderfully with the data formats
    that you’ll find in Elasticsearch. These reports can either be generated on-demand
    or they can be scheduled to run at a predefined interval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we’ve learned about an entire group of applications that make up
    what some call the Elastic SIEM. We’ve discussed the utilities we can use to collect,
    transport, enrich, and visually present security data. Now, it’s time to start
    setting up our technology to put it all into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the genealogy of Kali Linux and began to unpack
    one of the most popular toolsets offered by the Purple variant of that OS. We
    started to delve deeply into the Elastic Stack, sometimes called the ELK stack,
    as that will become one of the primary focal points of this book, being the core
    of Kali Purple. We gained a healthy understanding of how Elasticsearch and Logstash
    work with data, from ingesting it to enriching it to aggregating it. We also saw
    how this data can be presented visually through Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: After thoroughly examining how the ELK stack handles data, we started to examine
    the original data sources and how we glean data from there through Beats and pass
    it on to Elasticsearch, usually through Logstash as an intermediate stop, but
    not always. We studied the difference between sending data directly to Elasticsearch
    versus Logstash. We also peeked at the commercial component of the ELK stack,
    X-Pack, and were able to see how much of a full-fledged SIEM the ELK stack can
    become once X-Pack alerting is factored into the formula.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a robust understanding of the core abilities of Kali Purple,
    we can move on to the next phase of the learning process – practical application.
    In the next chapter, we will begin to set up our technology so that we can acquire,
    install, and configure our very own instance of Kali Purple. It’s time to take
    what we’ve been studying and put it into action!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to test your knowledge of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the ELK stack?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A herd of wild deer standing on top of each other
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A group of open source software working together
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Environmental Linux knowledge
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: Beats is a commercial product that is part of the ELK but costs
    money to use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary difference between a pipeline versus other aggregations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This type of aggregation utilizes the results of other aggregations
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Other aggregations depend on this one being conducted first
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This aggregation revolves around a long, thin linear set of criteria
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The EPA must be notified in the event of a pipeline breach
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Where is data enriched within the Elastic stack pipeline (you may select more
    than one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When one of the Beats agents collects the data before shipping it
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana only enriches the data if it is passed through both Logstash and Elasticsearch
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which ELK stack component helps the user visualize the data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Beats
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: X-Pack
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are conditionals as they relate to the ELK stack?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A device that processes the airflow, keeping it cool
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: An agreement between the operator of the Kali Purple instance and their customer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: An Elasticsearch process that only executes if all its demands are met
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A set of commands that will control the data flow based on whether predefined
    criteria are met
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the **BM25** **algorithm**: [https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables).'
  prefs: []
  type: TYPE_NORMAL
