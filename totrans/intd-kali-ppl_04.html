<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer026">
<h1 class="chapter-number" id="_idParaDest-37"><a id="_idTextAnchor039"/>2</h1>
<h1 id="_idParaDest-38"><a id="_idTextAnchor040"/>Kali Linux and the ELK Stack</h1>
<p>Now that we’ve gained a basic understanding of the evolution of cybersecurity as a professional field of study and practice, let’s begin to unpack the Kali Purple toolset. You’ll recall our explanation of red and blue colors creating purple on the color wheel. That’s because Kali Purple’s genealogy is a double-pronged utility coming from two suites of technical<a id="_idIndexMarker195"/> tools, one associated with the <strong class="bold">red team</strong> and the other with the <strong class="bold">blue team</strong>. We provided an overview<a id="_idIndexMarker196"/> of each grouping in the previous chapter. Those lists of tools were not nearly an exhaustive, or complete, list of tools – just <span class="No-Break">the highlights.</span></p>
<p>In this chapter, we are going to briefly explain Kali Linux for those who might be delving into Linux for the first time. A popular phenomenon has been developed with Kali Purple in that its defensive security offerings are causing some people to pursue experience with the Linux <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) for the first time in their careers. However, most folks who’ve picked up this book likely already have Linux experience, and some may even have Kali experience. So, while briefly covering Kali Linux, we will focus mostly<a id="_idIndexMarker197"/> on unpacking<a id="_idIndexMarker198"/> the initial elements of the <strong class="bold">security information and event management</strong> (<strong class="bold">SIEM</strong>) system, the <span class="No-Break"><strong class="bold">ELK stack</strong></span><span class="No-Break">.</span></p>
<p><strong class="bold">ELK</strong> stands for <strong class="bold">Elasticsearch, Logstash, and Kibana</strong>. They are three unique open source software offerings that are usually pasted together for technology solutions related to log management, data storage, search/query, data analysis, and data visualization. They alone do not define a SIEM. However, when combined with certain other core<a id="_idIndexMarker199"/> components, such as <strong class="bold">Beats</strong> and <strong class="bold">X-Pack</strong>, the ELK stack<a id="_idIndexMarker200"/> will deliver unto us the information and event management system we are <span class="No-Break">questing for.</span></p>
<p>There are multiple tools available in Kali Purple, but we will be focusing on the main powerhouse of the suite, the distributed<a id="_idIndexMarker201"/> analytics engine known as Elasticsearch. For us to understand the tools we will be working with, we will need to focus on the following <span class="No-Break">Elastic elements:</span></p>
<ul>
<li><span class="No-Break">Elasticsearch</span></li>
<li><span class="No-Break">Logstash</span></li>
<li><span class="No-Break">Kibana</span></li>
<li><span class="No-Break">Beats</span></li>
<li><span class="No-Break">X-Pack</span></li>
</ul>
<p>Since the SIEM is the core solution you will find within a SOC, it is critical that we fully explore and understand this tool, how it works, how and where it gets its data, and the different ways we can work with that data to accomplish our security objectives. The typical SIEM takes data from multiple other sources, normalizes it, enriches it, organizes it, stores it, and then presents it to cybersecurity analysts in a unified format for human evaluation and action. Organizations rely on it to centralize their defensive security operations. By the end of this chapter, you will have a solid understanding of <span class="No-Break">those concepts.</span></p>
<p>This understanding will come in handy throughout the next three chapters as we negotiate the process of preparing our technology for Kali Purple and then acquiring, installing, and configuring our technology along with Kali Purple itself. Knowing the data flow as it relates to the ELK stack and Kali Linux environment will provide the foundation you need to confront any anomalies that might arise when we unpack our <span class="No-Break"><em class="italic">SOC-in-a-Box</em></span><span class="No-Break"> solution.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>The evolution of <span class="No-Break">Kali Linux</span></li>
<li>Elasticsearch, Logstash, and Kibana (<span class="No-Break">ELK stack)</span></li>
<li>Agents <span class="No-Break">and monitoring</span></li>
</ul>
<h1 id="_idParaDest-39"><a id="_idTextAnchor041"/>The evolution of Kali Linux</h1>
<p>Though a groundbreaking<a id="_idIndexMarker202"/> technology, Kali Purple has its modern roots back in the year 1969. It was then in AT&amp;T’s Bell Laboratories that Ken Thompson and Dennis Ritchie co-created the UNIX OS. Ritchie is also famously known for creating the C programming language. Nearly all modern computing exists because of Ritchie’s two biggest contributions. The original Windows was programmed in C. UNIX eventually became available in both open source and commercial varieties. The two biggest customers of commercial UNIX were the United States <strong class="bold">Department of Defense</strong> (<strong class="bold">DOD</strong>) and – drumroll please – Apple Computer. That’s right. The Mac OS X is built from UNIX. That leaves only one major player to account <span class="No-Break">for… Linux.</span></p>
<p>A computer scientist at the University of Helsinki in Finland, Linus Torvalds grabbed one of the open source versions of UNIX and began to add his style to create a brand-new OS based on UNIX. He then publicly released the OS for free. <em class="italic">Linus + UNIX = Linux.</em> Because Mr. Torvalds released the code and a license allowing anybody to take it, modify it, and redistribute their own versions of it, Linux rapidly grew in popularity, with countless varieties of the OS available in the <span class="No-Break">world today.</span></p>
<p>The Kali Linux genealogy was reinforced by Dennis Ritchie a second time when he left Bell Labs to go work for the University of California at Berkley. It was there that he eventually created<a id="_idIndexMarker203"/> a newer version of UNIX that he dubbed <strong class="bold">Berkeley Software Distribution</strong> (<strong class="bold">BSD</strong>). It was this version of UNIX that Debian Linux – the OS based directly on Torvald’s initial contributions – utilized to beef up its own <span class="No-Break">powerful OS.</span></p>
<p>Highly reputable American information security company Offensive Security funded the development of Kali Linux to support ethical cybersecurity needs through digital forensics and penetration testing. Offensive Security employees Mati Aharoni and Devon Kearns took one of the company’s previous Linux distributions and revamped it to give the world the Debian Linux derivative now known as <span class="No-Break">Kali Linux.</span></p>
<p>As previously mentioned, the flagship<a id="_idIndexMarker204"/> of the SOC is the SIEM, and Kali Linux gave us the tools to put that together with the release of Kali Purple in December 2022. Let’s start examining the most potent of these tools, the <span class="No-Break">ELK stack.</span></p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor042"/>Elasticsearch, Logstash, and Kibana (ELK stack)</h1>
<p>The ELK stack refers<a id="_idIndexMarker205"/> to the three software components that collaboratively enrich, index, and visualize data for analysis. Some folks will include the Beats family of data collection agents as part of the ELK stack. We will cover Beats in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>Elasticsearch</h2>
<p>Elasticsearch is the central<a id="_idIndexMarker206"/> engine of the ELK stack. As we’ll discuss in a moment, Elasticsearch is technically a type of non-relational, NoSQL database. Without the ability to effectively search through the data we have collected, how on Earth would we ever be able to decide if there is something that should be alerted to? Kibana would have no path forward to properly display the visualization threats it provides to us. It is a very powerful component of the ELK stack in that it is designed to handle very large volumes of data in scenarios where a lot of querying <span class="No-Break">frequently occurs.</span></p>
<p>Elasticsearch accomplishes<a id="_idIndexMarker207"/> this by using a technique<a id="_idIndexMarker208"/> known as <strong class="bold">sharding</strong>. This is a database terminology that, in simple terms, allows data to be structured and organized by breaking it into pieces. This allows the technology to spread pieces of data horizontally across multiple partitions/nodes and servers while maintaining its relationship with similar data through indexing. These<a id="_idIndexMarker209"/> smaller chunks of data are called shards. The process of placing the data into shards occurs during the <span class="No-Break">indexing process.</span></p>
<p>The value of creating these shards is that Elasticsearch<a id="_idIndexMarker210"/> can then use a technique called <strong class="bold">parallel processing</strong> to access multiple shards simultaneously. Parallel processing is when more than one processing unit – a CPU, for example – works concurrently across multiple nodes. It provides the effect of having more than one computer working on the same thing at the same time to get the job done quicker. This is typically employed in <span class="No-Break">database technology.</span></p>
<p>Something beneficial Elasticsearch offers<a id="_idIndexMarker211"/> is the support for the creation of duplicates – <strong class="bold">replicas</strong> – of the shards. As implied, these are copies of the original shards. This is done to support the <em class="italic">availability</em> component of the cybersecurity framework by creating redundancy, which, in turn, serves the purpose of fault tolerance. It creates this safety net by placing the replica shards in a different node within the data cluster. Just as the original shards are created during the indexing process, so too are the creation of <span class="No-Break">the duplicates.</span></p>
<p>Although it may not sound brag-worthy to someone<a id="_idIndexMarker212"/> unfamiliar with searching technology, Elasticsearch offers <strong class="bold">full-text searching</strong>. That’s a big deal! Many searching/querying technologies only perform searches based on matching extracted keywords or embedded metadata. Full-text searching means that Elasticsearch will search for and retrieve results based on the entire contents of a document and text. That means considering the full contents of <span class="No-Break">the document.</span></p>
<p>There are six key features<a id="_idIndexMarker213"/> Elasticsearch utilizes to support this style <span class="No-Break">of searching:</span></p>
<ul>
<li><strong class="bold">Tokenization</strong>: Tokenization, when applied<a id="_idIndexMarker214"/> to full-text searching, might also be considered a form of sanitizing the text. What this means is that Elasticsearch breaks the text up into individual units. It then stores the small units, or tokens, in a reverse index – sometimes called an <strong class="bold">inverted index</strong>. That is done to also allow<a id="_idIndexMarker215"/> searching based on keywords or simple terms. Part of the tokenization process involves steps such as converting all uppercase characters into lowercase characters, removing punctuation, and dividing the input into <span class="No-Break">smaller units.</span></li>
<li><strong class="bold">Term-based queries</strong>: Term-based queries are just<a id="_idIndexMarker216"/> as they sound – a search that allows the users to state keywords or very specific terms they want Elasticsearch to seek out within a document. It accomplishes this by utilizing the tokenization method described previously to help narrow down the results, making sure they are directly relevant to the <span class="No-Break">search terms.</span></li>
<li><strong class="bold">Full-text matching</strong>: Elasticsearch utilizes mathematical algorithms to assist in result accuracy<a id="_idIndexMarker217"/> by employing a technique known as full-text matching. This is not to be confused with full-text searching, which we will talk about in a moment. Elasticsearch will attempt to determine the relevance of a document as it relates to the search query by counting the number of occurrences of terms in a document that match one of the search terms. It will<a id="_idIndexMarker218"/> also use algorithms such as <strong class="bold">Term Frequency-Inverse Documents Frequency</strong> (<strong class="bold">TF-IDF</strong>) and <strong class="bold">Best Match 25</strong> (<strong class="bold">BM25</strong>) to assess the importance<a id="_idIndexMarker219"/> of these search terms within the entire collection of documents searched because of measuring <span class="No-Break">those occurrences.</span></li>
<li><strong class="bold">Full-text search options</strong> (<strong class="bold">FTSO</strong>): Another style called FTSO<a id="_idIndexMarker220"/> is a related concept that differs in its focus and overall functionality. While the matching style focuses on exact matching, FTSO takes this a step further by also considering word proximity, term frequency, language analysis, relevance scoring as well as partial matches, synonyms, and fuzzy matches. It allows for Boolean operators commonly found in database languages (<strong class="source-inline">AND</strong>, <strong class="source-inline">OR</strong>, and <strong class="source-inline">NOT</strong>). In simple terms, the matching style tends to focus only on providing results where the words are an exact match with less of a consideration as to whether those results are relevant. FTSO places more effort into making sure the results aren’t simply word matches but relevant <span class="No-Break">content matches.</span></li>
<li><strong class="bold">Language analyzers</strong>: Have you ever wondered how technologies such as Elasticsearch deal with information that it finds in a language other than its native English? Believe it or not, it can accomplish that feat because Elasticsearch has integrated a variety of language analyzers. That includes analyzers that emphasize features other than spoken languages. Here are a few examples of some of the more<a id="_idIndexMarker221"/> popular language analyzers <span class="No-Break">Elasticsearch uses:</span><ul><li><strong class="bold">Standard analyzer</strong>: Grammar-based tokenization for <span class="No-Break">most languages</span></li><li><strong class="bold">Simple analyzer</strong>: Focuses on non-letter characters and <span class="No-Break">lowercase normalization</span></li><li><strong class="bold">Whitespace analyzer</strong>: Tokenizes each word that is separated <span class="No-Break">by whitespace</span></li><li><strong class="bold">Keyword analyzer</strong>: Tokenizes the entire <span class="No-Break">input string</span></li><li><strong class="bold">Spoken languages</strong>: English, French, German, Spanish, and <span class="No-Break">many others</span></li></ul></li>
<li><strong class="bold">Faceted searching and aggregations</strong>: Finally, Elasticsearch offers an advanced searching style<a id="_idIndexMarker222"/> known as faceted searching and aggregations, which involves complex<a id="_idIndexMarker223"/> data analysis. Faceted searching<a id="_idIndexMarker224"/> is sometimes called <strong class="bold">faceted navigation</strong>. This is a method that filters data based on different attributes of the data. These different filtering<a id="_idIndexMarker225"/> dimensions are sometimes called facets, hence the name. Some examples of these facets could be things such as color, price range, and shopping category. This style provides an avenue for the user to determine how they might like to refine their search. It allows the user to drill down to very precise levels <span class="No-Break">of searching.</span><p class="list-inset">Working together with faceted searching is aggregated searching. This is done at the beginning of the search to build the most robust supply of potentially relevant information for the faceted search to surgically discover the information most needed by the user. Three primary types of aggregations are utilized <span class="No-Break">by Elasticsearch:</span></p><ul><li><strong class="bold">Bucket aggregations</strong>: Using a logical partition<a id="_idIndexMarker226"/> to group documents with <span class="No-Break">related attributes</span></li><li><strong class="bold">Metric aggregations</strong>: Performs calculations<a id="_idIndexMarker227"/> on the data within <span class="No-Break">each bucket</span></li><li><strong class="bold">Pipeline aggregations</strong>: A secondary aggregation<a id="_idIndexMarker228"/> that operates based on output from <span class="No-Break">other aggregations</span></li></ul></li>
</ul>
<p>In addition to having a larger pool of considerations for the search, Elasticsearch also offers data indexing in near real time. One of the methods<a id="_idIndexMarker229"/> it uses to deliver that feature is something called <strong class="bold">inverted indexing</strong>. Whereas traditional indexing is when data is stored in a database and is organized based on the total document, inverted indexing organizes the data based on the words and terms within the documents. The index is built while they are initially parsed and analyzed before being placed into an optimized data structure for storage. That structure is the <span class="No-Break">index itself.</span></p>
<p>To further support near-real-time indexing, Elasticsearch<a id="_idIndexMarker230"/> first writes a document to a transaction log called a <strong class="bold">Write Ahead Log</strong> (<strong class="bold">WAL</strong>) immediately before the indexing process begins. This is done to protect the data in the case of a system failure. Even a brief hiccup in machine function could corrupt the indexing process. So, by using a WAL, Elasticsearch can immediately recover the data and begin the indexing <span class="No-Break">process again.</span></p>
<p>For the sake of data accuracy, Elasticsearch refreshes the indexes every second. When this occurs, it reinforces the availability of the information in the index for search operations. That process then gives way to near-real-time searching, making the overall process of retrieving information by Elasticsearch incredibly efficient in nature. This is a necessary component for the success of any application that depends on quick access to information for analytics, such as <span class="No-Break">an SIEM.</span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor044"/>Logstash</h2>
<p>Logstash is part of the ELK stack<a id="_idIndexMarker231"/> with capabilities for ingesting data, transforming/enriching it if needed, as well as transporting it from one place to another. While Elasticsearch is the core of the ELK stack, Logstash offers some parallel abilities and serves as the intermediary that <span class="No-Break">supplies Elasticsearch.</span></p>
<p>Logstash can ingest data from a vast array of data sources. Some of these sources<a id="_idIndexMarker232"/> might include <span class="No-Break">the following:</span></p>
<ul>
<li><span class="No-Break">Log files</span></li>
<li><span class="No-Break">Databases</span></li>
<li><strong class="bold">Application programming </strong><span class="No-Break"><strong class="bold">interfaces</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">APIs</strong></span><span class="No-Break">)</span></li>
<li><span class="No-Break">Message queues</span></li>
<li><strong class="bold">Internet of Things</strong> (<span class="No-Break"><strong class="bold">IoT</strong></span><span class="No-Break">) devices</span></li>
</ul>
<p>After ingesting the data, Logstash<a id="_idIndexMarker233"/> offers something called <strong class="bold">data transformation</strong> and <strong class="bold">data enrichment</strong>. This is a critical feature<a id="_idIndexMarker234"/> for any analytic application that alerts on anomalies. It performs these actions through the use of filters. These filters allow users to manipulate incoming data and then send it to the destination, where it will become accessible <span class="No-Break">to Elasticsearch.</span></p>
<p>There are many filters, around 50, but we are going to focus on some of the simpler and more popular filters here. Feel free to dig in and research new filters as you develop and hone your ELK stack skillset. In the meantime, let’s cover a few that folks will start learning and <span class="No-Break">training with.</span></p>
<p>One such filter is the <strong class="bold">Grok Filter</strong>. Logstash uses<a id="_idIndexMarker235"/> this to parse any type of structured log data – which Grok derives from unstructured data – that seems to follow a pattern. This action allows Logstash to also extract fields it deems meaningful from any unstructured or semi-structured log messages. It uses regular expressions to try and recognize and <span class="No-Break">define patterns.</span></p>
<p>The <strong class="bold">Date Filter</strong> is another transformation<a id="_idIndexMarker236"/> tool. As you might expect, this tool is used to parse and standardize incoming log data with date and time. You can customize the date format patterns you want it to parse. This filter is quite useful for any log files that contain any sort of date or timestamp. Any properly generated log file should have such stamps <span class="No-Break">on them.</span></p>
<p>There is also a <strong class="bold">Translate Filter</strong>, which performs data lookups<a id="_idIndexMarker237"/> against external mapping files. The filter uses this information to enrich the data coming in through the logs. This filter allows you to define key-value pairs within a dictionary file. The filter can then enrich the data by matching and replacing specific values with corresponding <span class="No-Break">dictionary values.</span></p>
<p>The aptly named <strong class="bold">Mutate Filter</strong> provides Logstash with a variety<a id="_idIndexMarker238"/> of operations that it can use to manipulate and modify data. With this filter, you can rename fields and add or remove them. You can also convert their types and modify the values contained within the fields. You can even do things such as split and trim strings or concatenate one string <span class="No-Break">with another.</span></p>
<p>If you want to add geographical information<a id="_idIndexMarker239"/> to the data, you will use the <strong class="bold">GeoIP Filter</strong>. It accomplishes this by performing IP address lookups. By mapping IP addresses to corresponding geographic locations, you can enrich your data with helpful information such as city, state, and country, as well as latitude <span class="No-Break">and longitude.</span></p>
<p>Logstash uses a <strong class="bold">User-Agent Filter</strong> to parse strings typically<a id="_idIndexMarker240"/> coming from weblogs, which enables it to extract important details about the tools used on the endpoint. This could include device type, OS, web browser brand, and version. Having this information can allow an analyst to gain a powerful insight into the devices and browsers used on either end of the <span class="No-Break">data flow.</span></p>
<p><strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) is one of the most commonplace<a id="_idIndexMarker241"/> code-related language formats found in defensive security tools. You are not likely to ever find a mainstream SIEM or other cybersecurity<a id="_idIndexMarker242"/> product that doesn’t have compatibility with JSON and for that matter, at least an option or two to present some of the information that’s utilized by the application in JSON format. Logstash is no exception. It can easily handle JSON formatted data and is even capable of handling nested JSON structures. A nested feature in any programming or coding environment is when such a feature is found included <span class="No-Break">within itself.</span></p>
<p>Spreadsheet fans will be excited to learn<a id="_idIndexMarker243"/> that Logstash offers a <strong class="bold">CSV Filter</strong>. This filter allows Logstash to parse and manipulate comma-separated values. That is the common format that’s used when a spreadsheet user extracts and/or stores data that can be presented in a spreadsheet manner. This filter handles a variety of configurations, such as delimiter, header, row, and <span class="No-Break">column mapping.</span></p>
<p>Another format that is both commonly found and compatible <a id="_idIndexMarker244"/>across different systems is <strong class="bold">eXtensible Markup Language</strong> (<strong class="bold">XML</strong>). Logstash has an XML filter to allow it to parse data that comes in XML formats. This filter can extract specific elements and/or attributes out of XML documents and then convert them into structured fields that can be used later <span class="No-Break">for processing.</span></p>
<p>The full collection of filters exceeds this list. These are just the most common and most likely to be used by Logstash. The complete filter collection makes Logstash a very robust data, normalization, and enrichment utility. As you might have picked up by now, Logstash is designed in a manner that allows it to deal with both structured and <span class="No-Break">unstructured data.</span></p>
<p>In dealing with all this data<a id="_idIndexMarker245"/> and all the different ways we’ve discussed how Logstash manipulates and enriches it, there is one critical coding style of a feature that it uses to control the data flow. Those who’ve worked with coding and/or software development before know of this type<a id="_idIndexMarker246"/> of data control flow as <strong class="bold">conditionals</strong>. Conditionals allow the user to control the flow based on certain specific conditions and criteria. Some might recognize these as an <span class="No-Break"><strong class="source-inline">if-else</strong></span><span class="No-Break"> syntax.</span></p>
<p>Here’s a <span class="No-Break">fun example:</span></p>
<pre class="source-code">
if (paycheck(onTime)){        // Set the condition. Am I paid on Time?
         doSomething;          // If I am, the condition is met
         beProductive;        // I will do something and be productive
} else {                    // Otherwise
     doNothing;            // I wasn't paid on time. Condition not met
     beLazy;               // I will be lazy and do nothing
}</pre> <p>As we’ve already touched on, Logstash is a bit of a liaison – a middleman or intermediary. That means it isn’t only relied upon for ingesting log information, it’s also heavily relied upon to enrich and forward that data with the primary output highway being Elasticsearch. It can output data in various formats but the most common are CSV and JSON, which we <span class="No-Break">discussed earlier.</span></p>
<p>There is a plugin for Logstash that C programmers will immediately<a id="_idIndexMarker247"/> recognize called <strong class="source-inline">stdout</strong> which stands for <strong class="bold">standard output</strong>. This plugin allows the data to be sent directly to the command line in console applications. The main reason for outputting data in that manner is to allow engineers working the Logstash instance to have the ability to immediately recognize certain data elements for debugging and application <span class="No-Break">testing purposes.</span></p>
<p>Another popular output destination is <strong class="bold">Apache Kafka</strong>, which is a distributed<a id="_idIndexMarker248"/> event streaming platform application that was originally developed by the popular social media platform LinkedIn. It was released as open source software under the Apache Software Foundation. It has earned a reputation for being a credible fault-tolerant and efficient utility capable of handling large volumes <span class="No-Break">of data.</span></p>
<p>Earlier, we talked about a means of data organization<a id="_idIndexMarker249"/> within Elasticsearch called the <strong class="bold">bucket</strong>. Logstash can also output to the organization that took this data organization principle and popularized it on a grand scale: Amazon. They<a id="_idIndexMarker250"/> did this via their <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) buckets. It is extremely popular in Amazon’s cloud<a id="_idIndexMarker251"/> services and is fully integrated with <strong class="bold">Amazon Web </strong><span class="No-Break"><strong class="bold">Services</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AWS</strong></span><span class="No-Break">).</span></p>
<p>Logz.io is a popular cloud-based log<a id="_idIndexMarker252"/> management platform that Logstash is compatible with outputting to. While it is a log management and analytics platform, it does not focus on real-time monitoring, correlation between raw and enriched data sources, and security event analysis. Therefore, it falls just a neuron short of being considered a <span class="No-Break">full-fledged SIEM.</span></p>
<p>Another Logstash output integration is <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>). JDBC is a Java API<a id="_idIndexMarker253"/> that allows developers to interact with relational databases using the SQL DB language. It provides interfaces and classes that are standard for performing database interactions such as connecting, executing SQL queries, updating, or <span class="No-Break">processing results.</span></p>
<p><strong class="bold">Redis</strong> is a similar storage option<a id="_idIndexMarker254"/> in that it can be used as a database, cache, or message broker. Logstash outputs this in-memory data structure storage as well. Something unique about Redis is that it stores its primary data in RAM to allow for lightning-fast reading and writing operations. It supports a very wide range of data types. It utilizes common database methods such as a key-value pair model. This is where the data is stored as a pair with a unique key to identify them. It focuses heavily on the use of keys and data structures such as strings, lists, sets, hashes, and sorted sets. The appeal of Redis<a id="_idIndexMarker255"/> is the incredibly fast and efficient way it manipulates, stores, and <span class="No-Break">recalls data.</span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor045"/>Kibana</h2>
<p>Kibana is the ELK stack<a id="_idIndexMarker256"/> utility that helps users explore, visualize, and analyze the data that’s been collected and aggregated by the other components. It is designed to specifically work with Elasticsearch to provide a user-friendly interface for the end user. The most direct<a id="_idIndexMarker257"/> term to describe Kibana is simply <span class="No-Break"><strong class="bold">data visualization</strong></span><span class="No-Break">.</span></p>
<p>Data visualization in Kibana occurs from many methods and styles but there are seven primary methods, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Charts</strong>: Charts are probably the most<a id="_idIndexMarker258"/> logical expectation<a id="_idIndexMarker259"/> here. They include line charts, bar charts, area charts, pie charts, scatter plots, and others. This gives the users who are more visual learners an opportunity to see potential anomalies from a <span class="No-Break">different perspective.</span></li>
<li><strong class="bold">Maps</strong>: While many software applications<a id="_idIndexMarker260"/> will make use of maps, Kibana<a id="_idIndexMarker261"/> takes this to a higher level. Users can create choropleth maps, symbol maps, and maps based on grid coordinates. A <strong class="bold">choropleth map</strong> is a type of thematic map<a id="_idIndexMarker262"/> that focuses on a specific theme or topic. The choropleth style is to use different colors or patterns to represent geographical areas/regions. Usually, that would be countries, states/provinces, or counties/shires. You have probably seen many of these types of maps in your childhood classrooms. Symbol maps are similar but instead focus on using symbols to identify and represent specific geographical features, while grid coordinate maps are popular when aligned with latitude <span class="No-Break">and longitude.</span></li>
<li><strong class="bold">Timelion</strong>: Timelion, as the name <a id="_idIndexMarker263"/>suggests, is a data<a id="_idIndexMarker264"/> visualization plugin for Kibana that allows end users to manipulate and analyze time-based data. Like most utilities associated with the ELK stack, Timelion can be used for data transformation, aggregations, and mathematical calculations. You can perform averages, percentiles, and sums, among other functions. It allows the use of variables, which is handy in dealing with irregular time series. The typical visualization that’s created by Timelion comes in the form of a line chart. When applied to technology, line charts are nearly always present in the form of an <em class="italic">X</em> and <em class="italic">Y</em>-axis, where the <em class="italic">X</em>-axis is representative of time<a id="_idIndexMarker265"/> and the <em class="italic">Y</em>-axis represents<a id="_idIndexMarker266"/> the <span class="No-Break">data values.</span></li>
<li><strong class="bold">Time Series Visual Builder</strong> (<strong class="bold">TSVB</strong>): The TSVB is a visualization option in Kibana that focuses<a id="_idIndexMarker267"/> on the end user’s comfort. It allows<a id="_idIndexMarker268"/> them to build complicated time series visualizations with a simple drag-and-drop interface. It gives access to a variety of chart types, mathematical aggregations, and <span class="No-Break">custom styling.</span></li>
<li><strong class="bold">Gauge and metric</strong>: Kibana’s version of gauge and metric<a id="_idIndexMarker269"/> visualizations is directed<a id="_idIndexMarker270"/> at providing visuals that represent a single value within a specific range. This is to allow users to present such simple values in a variety of ways, including sparklines. <strong class="bold">Sparklines</strong> are small and condensed<a id="_idIndexMarker271"/> line bar charts or sometimes simple line charts that are meant to display variations in data or trends within a condensed space. They’re meant to give a quick visual <em class="italic">shape</em> or look at <span class="No-Break">the data.</span></li>
<li><strong class="bold">Heat maps</strong>: One of the more popular<a id="_idIndexMarker272"/> data visualizations<a id="_idIndexMarker273"/> in the business world today is heat maps. Heat maps are two-dimensional and often present in the form of a global or other large geographical map. They are popularly used to display trends, correlations, and patterns to help analysts solve complicated and/or wide-ranging problems. One of the more popular heat map usages in the world of cybersecurity is to show the origins of cyberattacks or source IP locations. Typically, heat maps have carefully integrated color schemes so that when multiple entities cross paths, it increases the intensity of the color, such as when some mobile phone GPS maps darken the route color from blue to yellow and eventually to red, depending on how congested the traffic is in a particular location. Heat maps use labels and annotations, but the best ones will keep this concise and limited<a id="_idIndexMarker274"/> to keep the visualization appearing<a id="_idIndexMarker275"/> clean to <span class="No-Break">the observer.</span></li>
<li><strong class="bold">Tag clouds</strong>: Tag clouds, also known as <strong class="bold">word clouds</strong>, are visualizations of text data that is usually<a id="_idIndexMarker276"/> shown with words in different<a id="_idIndexMarker277"/> sizes and colors<a id="_idIndexMarker278"/> based on category and prominence. They tend to measure word prominence based on frequency. The algorithms that generate these sorts of maps usually perform a bit of pre-processing before deciding on the final output. They do this to remove common words that are not likely related to data, such as prepositions and pronouns. This is a form of text sanitization, similar to tokenization, which is seen in Elasticsearch <span class="No-Break">text-matching methods.</span></li>
</ul>
<p>Another type of visualization<a id="_idIndexMarker279"/> offered by Kibana is <strong class="bold">dashboard creation</strong>. This is the overarching element that is most<a id="_idIndexMarker280"/> associated with Kibana and the one that makes use of the different charting and other visualizations we’ve just explored. A Kibana dashboard provides a centralized view where multiple visualizations can be combined, and queries and filters can be applied to create a fully customized layout. They help track<a id="_idIndexMarker281"/> what is known in the business world as <strong class="bold">key performance metrics</strong> (<strong class="bold">KPIs</strong>) in near <span class="No-Break">real time.</span></p>
<p>Kibana provides support for users to perform ad hoc searches. This is done across indexed data in Elasticsearch but with the benefit of a visual search interface. You can construct queries using simple search syntax if you like. The more advanced users have the option of using the Lucene query syntax, which is helpful for precise searches. The search results are displayed instantly and then you can further refine, filter, and sort the data if you wish to continue drilling down into specific subsets <span class="No-Break">of information.</span></p>
<p>Speaking of queries and filters, Kibana provides a way to visually build queries and filters so that you can deeply explore data. Kibana’s <strong class="bold">domain-specific language</strong> (<strong class="bold">DSL</strong>) allows users<a id="_idIndexMarker282"/> to construct complex queries and aggregations to extract very deep and meaningful insights from the data. You can use filters to narrow down data based on specific criteria, such as time range, attributes, or <span class="No-Break">custom-defined conditions.</span></p>
<p>One of the greater strengths Kibana has is its ability to excel at time-based analysis. This allows you to explore data trends, patterns, and anomalies more thoroughly over a certain period. You can adjust the time ranges, set custom time intervals, and even apply histograms with dates so that you can analyze data at different levels of granularity. The time picker enables you to specify the time range you wish <span class="No-Break">to analyze.</span></p>
<p><strong class="bold">Geospatial analysis</strong> is another feature that’s offered<a id="_idIndexMarker283"/> that allows you to visualize and analyze location-based data. You can use this feature to plot data on interactive maps, geocode IP addresses, apply geospatial aggregations, and visualize density using heatmaps. A nice bonus from Kibana is the support for multiple map layers, custom base maps, and geographic shape files. It also offers geospatial extensions, the most common being the Elastic <span class="No-Break">Maps Service.</span></p>
<p>Kibana allows users to set up alerts and monitors to track specific events or conditions in real-time data. You can define alert rules based on a variety of factors, including data thresholds, patterns, or other conditions. It can also work with external notifications such as email and the popular webhook, among others, as the alerts are triggered. It is this set of features that takes the ELK stack one step closer to an authentic SIEM environment than most log ingestion and <span class="No-Break">manipulation suites.</span></p>
<p>Kibana isn’t only a data visualization tool – it also offers an element of security. It provides a very robust set of security features to control access to data. It does this by supporting<a id="_idIndexMarker284"/> authentication<a id="_idIndexMarker285"/> and authorization mechanisms, <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>), and integration with external<a id="_idIndexMarker286"/> authentication providers such as <strong class="bold">Lightweight Directory Access Protocol</strong> (<strong class="bold">LDAP</strong>) and/or the <strong class="bold">Security Assertion Markup </strong><span class="No-Break"><strong class="bold">Language</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SAML</strong></span><span class="No-Break">).</span></p>
<p>The ELK stack, as presented<a id="_idIndexMarker287"/> thus far, works with data manipulation/enrichment, visualization, and overall processing. However, we need to have a way of gathering this information and sending it to the ELK stack in the first place. <span class="No-Break">Enter Beats.</span></p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor046"/>Agents and monitoring</h1>
<p>There are a couple of prominent additional applications that are associated with the ELK stack. They are the open source Beats and the commercially available X-Pack. It is the addition of these two components that transitions the ELK stack into a fully functioning SIEM. Together, they provide data collection and shipping, alerting and notification, machine learning for detecting hidden anomalous behavior, and <span class="No-Break">automated reporting.</span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor047"/>Beats</h2>
<p>Beats is a group of data collection<a id="_idIndexMarker288"/> and transportation<a id="_idIndexMarker289"/> agents. These are sometimes referred to as <strong class="bold">data shippers</strong>. They are lightweight – miniature – applications that are installed on endpoints so that they include personal computers, servers, and other network devices for the sole purpose of collecting data to ship off to Elasticsearch and Logstash for further processing in real time. Beats collect operational data from the devices they are installed on. It gets it from different sources, including logs and network traffic, along with other relevant information. The Beats family includes various specialized<a id="_idIndexMarker290"/> data shippers, such as <strong class="bold">Filebeat</strong> for log files, <strong class="bold">Metricbeat</strong> for machine metrics, <strong class="bold">Packetbeat</strong> for network data, and <strong class="bold">Auditbeat</strong> for audit events, among<a id="_idIndexMarker291"/> others. Beats<a id="_idIndexMarker292"/> uses lightweight<a id="_idIndexMarker293"/> agents installed on servers or systems to efficiently collect data with minimal resource usage. Let’s look at these in <span class="No-Break">some detail:</span></p>
<ul>
<li><strong class="bold">Filebeat</strong>: Filebeat is designed for collecting<a id="_idIndexMarker294"/> and shipping log files. It can monitor log directories and files. It takes the data that it collects and ships it off to Elasticsearch or Logstash, where it will be enriched and/or further processed and analyzed. Filebeat supports different log file formats, including plain text logs, JSON logs, syslog, <span class="No-Break">and others.</span></li>
<li><strong class="bold">Metricbeat</strong>: This beat collects system-level metrics<a id="_idIndexMarker295"/> and statistics, such as CPU usage, memory utilization, disk I/O, and network metrics among others. It can gather metrics from various sources, such as OSs, services, containers, databases, and cloud platforms. Just like Filebeat operates, Metricbeat ships the collected metrics to Elasticsearch or Logstash for enrichment and further processing, storage, analysis, <span class="No-Break">and visualization.</span></li>
<li><strong class="bold">Packetbeat</strong>: Wouldn’t it be great to capture<a id="_idIndexMarker296"/> these types of metrics from network data? That’s where Packetbeat comes into play. It captures network data and analyzes various protocols, including HTTP, DNS, MySQL, and Redis, along with many other protocols so that it can extract metadata and behavioral insights. Packetbeat’s data collection targets mean that it can provide near-real-time visibility into network traffic. This might include transaction details, request and response data, or network errors. Packetbeat can also be used for troubleshooting performance issues, identifying security threats, and monitoring <span class="No-Break">network activity.</span></li>
<li><strong class="bold">Auditbeat</strong>: This one is designed to collect<a id="_idIndexMarker297"/> and ship audit events from the Linux Audit Framework and Windows Event Logs. Auditbeat monitors system logs for audit events so that it can provide insight into user activities and look for unauthorized escalation of privileges by monitoring privilege changes, filesystem modifications, and others. Auditbeat has the potential to be a key player in identifying a compromised network and/or device. It is an invaluable tool that helps with compliance monitoring, security analysis, and detecting <span class="No-Break">suspicious activities.</span></li>
</ul>
<p>As a package, Beats<a id="_idIndexMarker298"/> allows for some data transformation capabilities so that it can prepare the collected data for analysis. Beats provides lightweight functions that are applied to events before they’re sent on to Elasticsearch<a id="_idIndexMarker299"/> or Logstash. These functions are called Beats processors. They can enrich, filter, and modify the collected data. These processors allow operations such as filtering fields, enriching data with metadata, renaming fields, and much, <span class="No-Break">much more.</span></p>
<p>One critical consideration for business technologies is the ability to adapt to sudden and/or unexpected rapid business growth. Thankfully, Beats is built to handle scalability. Not only that, but it is also designed to operate in high-volume and distributed environments. Beats can be easily scaled horizontally through the deployment of multiple Beats agents across different servers or systems. These agents provide configurable options to deal with load balancing, fault tolerance, and failover. This ensures data collection with high availability <span class="No-Break">and resilience.</span></p>
<p>Being an integral contribution to the Elastic stack, Beats’ design allows it to seamlessly integrate with the other components of the ELK stack – Elasticsearch, Logstash, and Kibana. They enable the efficient collection, transport, and processing of data before it is stored in Elasticsearch for further analysis and visualization. Beats can be configured to send data directly to Elasticsearch or Logstash for additional data transformation <span class="No-Break">and enrichment.</span></p>
<p>That last part brings up a topic we should address. We’ve discussed a few times now that data can be sent directly to either Elasticsearch or Logstash and that Beats is also capable of sending it to either one. It’s important to examine these scenarios so that as we advance throughout our careers, we can select the appropriate configurations when setting up the <span class="No-Break">data shippers.</span></p>
<p>That said, let’s consider that sending<a id="_idIndexMarker300"/> data directly to Elasticsearch or instead to Logstash as an intermediary both has its advantages <span class="No-Break">and disadvantages.</span></p>
<p>The advantages of sending data directly to Elasticsearch<a id="_idIndexMarker301"/> are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Simplicity</strong>: Sending data directly to Elasticsearch means there’s one less stop the data needs to make before reaching its destination. By eliminating Logstash as an intermediary, we are keeping the data <span class="No-Break">pipeline simplified.</span></li>
<li><strong class="bold">Scalability</strong>: As we talked about just a short while ago, modern technology in business needs to be able to adjust to sudden and/or unexpected business growth. By directly sending data to Elasticsearch, we are creating an allowance for horizontal scaling as well. That will make it possible to distribute the workload across multiple Elasticsearch nodes. This significantly improves performance and gives us the ability to handle larger volumes <span class="No-Break">of data.</span></li>
<li><strong class="bold">Efficiency</strong>: Any time a stop is removed from a data flow, less processing overhead will be required. There is also going to be less lag/latency due to the ability of the data to keep moving past the point where a stop would otherwise be located. This gives us the near-real-time data ingestion and indexing of data that Elasticsearch is known for. That, in turn, leads to faster availability for search <span class="No-Break">and analysis.</span></li>
<li><strong class="bold">Streamlined architecture</strong>: Supporting the greater efficiency argument, having Beats, or any data shipper for that matter, make a direct integration with Elasticsearch means we have a tighter and more straightforward architecture. That keeps the entire data flow simpler, which, in addition to improved performance, also means<a id="_idIndexMarker302"/> less surface for breaks <span class="No-Break">and flaws.</span></li>
</ul>
<p>The disadvantages of sending<a id="_idIndexMarker303"/> data directly to Elasticsearch are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Limited data transformation</strong>: We may be more efficient but we are also missing out on the benefits of Logstash. This means that by sending data directly to Elasticsearch, we no longer have the data enrichment and transformation capabilities that Logstash has to offer. Elasticsearch still has these values but not at the same level as Logstash. Logstash provides much more powerful filtering, enrichment, and data parsing capabilities so that it can preprocess and transform the data before <span class="No-Break">indexing it.</span></li>
<li><strong class="bold">Complexity in certain scenarios</strong>: There are going to be scenarios where the additional preprocessing of data would simplify the work Elasticsearch needs to do in the end. So, there are times when there will be enrichment that is better suited for Logstash. Ultra-complex data pipelines, especially those involving many data sources or more advanced transformations, would benefit better from the flexibility and capabilities <span class="No-Break">of Logstash.</span></li>
<li><strong class="bold">Compatibility with non-Elasticsearch systems</strong>: There could be times when systems other than Logstash or possibly even other than Beats will ship data to Elasticsearch. If any of these systems do not integrate fully with Elasticsearch, there could be difficulties with normalizing the data being transferred. Any data coming from Logstash is certain to be compatible with Elasticsearch. Logstash, on the other hand, is set up with a multitude of plugins to assist with transforming data into a format that is compatible <span class="No-Break">with Elasticsearch.</span></li>
</ul>
<p>The advantages of using Logstash<a id="_idIndexMarker304"/> as an intermediary are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Data transformation and enrichment</strong>: Logstash possesses very robust data processing capabilities that are simply greater in size and scope than Elasticsearch has to offer. Logstash is capable of advanced filtering, enriching, and parsing of data. It also has some alerting capabilities. It can support the more advanced styles due to its vast array of plugins and filters. Collectively, these abilities allow Logstash to transform data into a compatible format before passing it <span class="No-Break">onto Elasticsearch.</span></li>
<li><strong class="bold">Compatibility and integration</strong>: The vast array of plugins and filters that exist in Logstash allows for integration with many data sources and destinations. Logstash can support many different formats, protocols, and systems because of this. That also makes it compatible with many non-Elasticsearch systems, which it can grab data from and translate for Elasticsearch. This gives us what some would call data <span class="No-Break">pipeline versatility.</span></li>
<li><strong class="bold">Flexibility in complex scenarios</strong>: Whereas Elasticsearch is not the best suited for extremely complex scenarios, Logstash excels in this area. These areas include complex data pipelines, and situations where data needs to be collected from multiple sources, transformed, combined, or split before being passed on to Elasticsearch. This increases overall granularity and flexibility when dealing with diverse requirements and <span class="No-Break">data manipulation.</span></li>
</ul>
<p>The disadvantages of using Logstash<a id="_idIndexMarker305"/> as an intermediary are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Increased complexity</strong>: While Logstash does expand the ability to normalize many different types of data for Elasticsearch, it also introduces an additional stop in the data flow by adding a new component to the pipeline. That’s one more thing that must be configured, managed, and refined. This can increase the complexity and overhead of the system, which is especially concerning for smaller deployments that likely have <span class="No-Break">simpler requirements.</span></li>
<li><strong class="bold">Potential performance impact</strong>: Anytime an additional component is added to any technological pipeline, this means there will be an additional need for processing and overhead. This will reduce efficiency and increase the latency/lag of the data flow. In cases where there is a large volume of data flow and/or a large number of transformations and enrichments needed, there is likely to be a <span class="No-Break">performance hit.</span></li>
<li><strong class="bold">Overhead and resource utilization</strong>: Because there is extra technology, there is also going to be extra demand for the resources. That means more stress on the CPU, memory, and disk space compared to sending the data directly to the Elasticsearch integration. This might be more valuable while considering scenarios involving very high volumes of data or if the previously mentioned resources<a id="_idIndexMarker306"/> <span class="No-Break">are limited.</span></li>
</ul>
<p>The general gist of Elasticsearch versus Logstash data ingestion is that sending data directly to Elasticsearch improves efficiency while reducing capabilities and sending it to Logstash first has the exact opposite effect of increasing capabilities while reducing efficiency. Elasticsearch greatly improves the overall efficiency and reduces preprocessing overhead. Logstash offers a plethora of data transformations where it can take data not compatible with Elasticsearch in its native format. Then, when it’s done processing it, it delivers it to Elasticsearch as a compatible ingestion platform. In the end, the decision about where to send the data is going to depend on how you feel about the expected complexity of the data pipeline, the need for data transformation, and compatibility with other systems in <span class="No-Break">your environment.</span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor048"/>X-Pack</h2>
<p>Whereas the four components<a id="_idIndexMarker307"/> of the ELK stack we’ve discussed thus far are all free, open source applications, X-Pack is a partially free extension that offers additional paid services. It offers enhanced capabilities over the completely <span class="No-Break">free applications.</span></p>
<p>It has five <span class="No-Break">distinct</span><span class="No-Break"><a id="_idIndexMarker308"/></span><span class="No-Break"> offerings:</span></p>
<ul>
<li><strong class="bold">Security</strong>: X-Pack delivers security through several actions. One offering is RBAC, which allows you to define and manage roles and privileges for users. This provides a strict level of control over documents, indexes, and other resources. It provides <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) encryption for secure communication<a id="_idIndexMarker309"/> between clients and Elasticsearch. X-Pack also offers secure user authentication by utilizing methods such as LDAP, Active Directory, and native authentication. This helps keep access to the Elasticsearch cluster safe and secure. It also has an audit logging feature so that it can log security-related events to help you monitor and track user and system actions for <span class="No-Break">anomalous behavior.</span></li>
<li><strong class="bold">Monitoring</strong>: X-Pack provides detailed<a id="_idIndexMarker310"/> monitoring of Elasticsearch clusters. Some of the issues it monitors include node health, resource usage, and indexing rates. This is so that you can identify and respond to performance issues and bottlenecks. It will also look at query performance, identifying slow-responding queries and inefficient search patterns. You can set up monitoring alerts based on predefined conditions and thresholds. These alerts will provide notifications to you if a predefined condition or metric <span class="No-Break">is met.</span></li>
<li><strong class="bold">Alerting</strong>: One of the most critical components of X-Pack is its alerting capabilities. It<a id="_idIndexMarker311"/> uses an application called <strong class="bold">Watcher</strong> to define and schedule alert rules based on query results and aggregations. Like all the big players in SIEM technology, it can perform actions such as providing automatic email notifications or Slack messages. It can also execute custom scripts, giving analysts the power to proactively respond to <span class="No-Break">critical events.</span></li>
<li><strong class="bold">Machine learning (ML)</strong>: X-Pack offers anomaly detection<a id="_idIndexMarker312"/> through its ML capability. This enables the automatic detection of anomalies in time series data, which assists analysts in identifying unusual patterns, deviations, or other anomalous behavior. It also uses ML to forecast future trends based on historical patterns that would otherwise be hard to detect or are hidden altogether. Something interesting about X-Pack’s ML is that it helps to identify what it believes to be the most influential factors that contribute to anomalous behavior. This helps investigators and incident responders with their examination of root <span class="No-Break">cause analysis.</span></li>
<li><strong class="bold">Reporting</strong>: Finally, X-Pack allows you to create and schedule reports, which is a well-rounded feature that, like alerting, will help transition your ELK stack into a full-fledged SIEM. It offers these reports in a variety of formats, such as <strong class="source-inline">.pdf</strong>, <strong class="source-inline">.csv</strong>, and <strong class="source-inline">.xls</strong>, which works wonderfully with the data formats that you’ll find in Elasticsearch. These reports can either be generated on-demand or they can be scheduled to run<a id="_idIndexMarker313"/> at a <span class="No-Break">predefined interval.</span></li>
</ul>
<p>With that, we’ve learned about an entire group of applications that make up what some call the Elastic SIEM. We’ve discussed the utilities we can use to collect, transport, enrich, and visually present security data. Now, it’s time to start setting up our technology to put it all <span class="No-Break">into practice.</span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor049"/>Summary</h1>
<p>In this chapter, we learned about the genealogy of Kali Linux and began to unpack one of the most popular toolsets offered by the Purple variant of that OS. We started to delve deeply into the Elastic Stack, sometimes called the ELK stack, as that will become one of the primary focal points of this book, being the core of Kali Purple. We gained a healthy understanding of how Elasticsearch and Logstash work with data, from ingesting it to enriching it to aggregating it. We also saw how this data can be presented visually <span class="No-Break">through Kibana.</span></p>
<p>After thoroughly examining how the ELK stack handles data, we started to examine the original data sources and how we glean data from there through Beats and pass it on to Elasticsearch, usually through Logstash as an intermediate stop, but not always. We studied the difference between sending data directly to Elasticsearch versus Logstash. We also peeked at the commercial component of the ELK stack, X-Pack, and were able to see how much of a full-fledged SIEM the ELK stack can become once X-Pack alerting is factored into <span class="No-Break">the formula.</span></p>
<p>Now that we have a robust understanding of the core abilities of Kali Purple, we can move on to the next phase of the learning process – practical application. In the next chapter, we will begin to set up our technology so that we can acquire, install, and configure our very own instance of Kali Purple. It’s time to take what we’ve been studying and put it <span class="No-Break">into action!</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor050"/>Questions</h1>
<p>Answer the following questions to test your knowledge of <span class="No-Break">this chapter:</span></p>
<ol>
<li>What is the <span class="No-Break">ELK stack?</span><ol><li class="Alphabets">A herd of wild deer standing on top of <span class="No-Break">each other</span></li><li class="Alphabets">A group of open source software <span class="No-Break">working together</span></li><li class="Alphabets">Environmental <span class="No-Break">Linux knowledge</span></li></ol></li>
<li>True or false: Beats is a commercial product that is part of the ELK but costs money <span class="No-Break">to use.</span><ol><li class="Alphabets"><span class="No-Break">True</span></li><li class="Alphabets"><span class="No-Break">False</span></li></ol></li>
<li>What is the primary difference between a pipeline versus <span class="No-Break">other aggregations?</span><ol><li class="Alphabets">This type of aggregation utilizes the results of <span class="No-Break">other aggregations</span></li><li class="Alphabets">Other aggregations depend on this one being <span class="No-Break">conducted first</span></li><li class="Alphabets">This aggregation revolves around a long, thin linear set <span class="No-Break">of criteria</span></li><li class="Alphabets">The EPA must be notified in the event of a <span class="No-Break">pipeline breach</span></li></ol></li>
<li>Where is data enriched within the Elastic stack pipeline (you may select more <span class="No-Break">than one)?</span><ol><li class="Alphabets">When one of the Beats agents collects the data before <span class="No-Break">shipping it</span></li><li class="Alphabets">Kibana only enriches the data if it is passed through both Logstash <span class="No-Break">and Elasticsearch</span></li><li class="Alphabets"><span class="No-Break">Logstash</span></li><li class="Alphabets"><span class="No-Break">Elasticsearch</span></li></ol></li>
<li>Which ELK stack component helps the user visualize <span class="No-Break">the data?</span><ol><li class="Alphabets"><span class="No-Break">Elasticsearch</span></li><li class="Alphabets"><span class="No-Break">Logstash</span></li><li class="Alphabets"><span class="No-Break">Kibana</span></li><li class="Alphabets"><span class="No-Break">Beats</span></li><li class="Alphabets"><span class="No-Break">X-Pack</span></li></ol></li>
<li>What are conditionals as they relate to the <span class="No-Break">ELK stack?</span><ol><li class="Alphabets">A device that processes the airflow, keeping <span class="No-Break">it cool</span></li><li class="Alphabets">An agreement between the operator of the Kali Purple instance and <span class="No-Break">their customer</span></li><li class="Alphabets">An Elasticsearch process that only executes if all its demands <span class="No-Break">are met</span></li><li class="Alphabets">A set of commands that will control the data flow based on whether predefined criteria <span class="No-Break">are met</span></li></ol></li>
</ol>
<h1 id="_idParaDest-49"><a id="_idTextAnchor051"/>Further reading</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <strong class="bold">BM25 </strong><span class="No-Break"><strong class="bold">algorithm</strong></span><span class="No-Break">: </span><a href="https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables"><span class="No-Break">https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables</span></a><span class="No-Break">.</span></p>
</div>
</div></body></html>